#!/usr/bin/env python3.6

import argparse
import glob
import os
import datetime
import dateparser
import tempfile
from elasticsearch import Elasticsearch
from elasticsearch.helpers import scan
import gzip
import shutil
from transliterate import translit

maxEntries = 50000

filePrefix = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
"""

entryTemplate = """<url>
 <loc>{url}</loc>
 <lastmod>{lastmod}</lastmod>
 <changefreq>weekly</changefreq>
</url>"""

fileSuffix = """
</urlset>
"""

robotsTxt = """User-Agent: *
Disallow:

"""


def makeUrl(path, doc):
    print(doc)
    title = translit(doc['title'], 'bg', reversed=True)
    return f'https://naematel.com/{path}/{title}'


def addEntry(handle, doc, outputDir):
    lastmod = dateparser.parse(doc.get('time', doc.get('import_date', datetime.datetime.now()))).date().isoformat()
    url = makeUrl(doc["_id"], doc)
    entry = entryTemplate.format(url=url, lastmod=lastmod)
    if not handle[0]:
        newFilename = f'sitemap-{handle[1]}.xml'
        print('Writing in:', newFilename)
        handle[0] = open(os.path.join(outputDir, newFilename), 'w')
        handle[1] += 1
        handle.append(newFilename)
        handle[0].write(filePrefix)

    handle[0].write(entry)
    return handle


def genSitemap(outputDir, index):
    count = 0
    handle = [None, 1]
    for doc in scan(es, index=index, doc_type='ad'):
        doc['_source']['_id'] = doc['_id']
        addEntry(handle, doc['_source'], outputDir)
        count += 1
        if count >= maxEntries:
            handle[0].write(fileSuffix)
            handle[0].close()
            handle[0] = None
            count = 0

    if handle[0]:
        handle[0].write(fileSuffix)
        handle[0].close()

    sitemapFiles = handle[2:]
    robots = f'robots.txt'
    print('Writing in:', robots)
    with open(os.path.join(outputDir, robots), 'w') as f:
        f.write(robotsTxt)
        for sm in sitemapFiles:
            url = makeUrl(sm)
            f.write(f'Sitemap: {url}\n')

    allFiles = sitemapFiles + [robots]
    return allFiles


def removeSitemaps(outDir):
    for fmt in ['xml', 'txt']:
        for f in glob.glob(os.path.join(outDir, 'sitemap-*.' + fmt)):
            print('Remove:', f)
            os.unlink(f)


def compressFiles(outputDir, files):
    for f in files:
        filePath = os.path.join(outputDir, f)
        with open(filePath, 'rb') as f_in:
            with gzip.open(f'{filePath}.gz', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)


usage = "usage: %prog <feed>"
description = "Generate sitemap files"
parser = argparse.ArgumentParser(description=description)
parser.add_argument("out",
                    help="output directory")
parser.add_argument("-i", "--index",
                    default='rentbot-all',
                    help="Index name")
args = parser.parse_args()
outParent = os.path.dirname(os.path.abspath(args.out).rstrip('/'))
tmpDir = tempfile.mkdtemp(dir=outParent)

es = Elasticsearch(['localhost:9200'])

print('Create dir:', tmpDir)
os.makedirs(tmpDir, exist_ok=True)
os.chmod(tmpDir, 0o755)

allFiles = genSitemap(tmpDir, args.index)
compressFiles(tmpDir, allFiles)
print('Remove:', args.out)
shutil.rmtree(args.out)
print(f'Rename {tmpDir} -> {args.out}')
os.rename(tmpDir, args.out)
##
