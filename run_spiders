#!/usr/bin/env python3.6

import os
import datetime
import subprocess

SPIDERS = {
    'olx': None,
    'imot': None,
    'fbGroup': [
        'https://m.facebook.com/groups/kvartiti.pod.naem/',
        'https://m.facebook.com/groups/apartamenti.sofia/',
    ]
}
STOP_AFTER_SKIPPED = 10

THIS_DIR = os.path.dirname(os.path.realpath(__file__))
DATA_DIR = '/srv/'
DATA_DIR = './_test/'


RUN_DIR = DATA_DIR + 'crawl/' + datetime.datetime.now().strftime("%Y-%m-%dT%H:%M")

os.makedirs(RUN_DIR, exist_ok=True)


def runSpider(spiderName, url, full=False):
    feedFile = f'{spiderName}.jl'
    feedPath = os.path.join(RUN_DIR, feedFile)
    logPath = os.path.join(RUN_DIR, f'{spiderName}.log')
    command = [
        #'echo',
        'scrapy',
        'crawl', spiderName,
        '--set', f'FEED_URI=file://{feedPath}',
        '--set', 'DELTAFETCH_ENABLED=1',
    ]
    if not full:
        command += [
            '--set', 'STOP_AFTER_SKIPPED={STOP_AFTER_SKIPPED}',
        ]
    if full:
        command += [
            '--set', 'DELTAFETCH_RESET=1'
        ]
    if url:
        command += [f' --set START_URL={url}']
    with open(logPath, 'a+') as log:
        subprocess.run(command, stdout=log, stderr=log)
    return feedPath


def importFeeds(feeds):
    command = [
        os.path.join(THIS_DIR, "import_feed"),
    ]
    command += feeds
    logPath = os.path.join(RUN_DIR, 'import.log')
    with open(logPath, 'a+') as log:
        subprocess.run(command, stdout=log, stderr=log)


feeds = []
for spiderName, urls in SPIDERS.items():
    if urls:
        for url in urls:
            feedPath = runSpider(spiderName, url)
            feeds.append(feedPath)
    else:
        feeds = [runSpider(spiderName, None)]

importFeeds(list(set(feeds)))
