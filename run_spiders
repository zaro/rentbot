#!/usr/bin/env python3.6

import os
import datetime
import subprocess
import argparse

SPIDERS = {
    'olx': None,
    'imot': None,
    'fbGroup': [
        'https://m.facebook.com/groups/kvartiti.pod.naem/',
        'https://m.facebook.com/groups/apartamenti.sofia/',
    ]
}
STOP_AFTER_SKIPPED = 10

crawlDate = datetime.datetime.now()  # - datetime.timedelta(days=1)


THIS_DIR = os.path.dirname(os.path.realpath(__file__))
DATA_DIR = '/srv/'
DATA_DIR = os.path.join(os.getcwd(), '_test')


RUN_DIR = os.path.join(DATA_DIR, 'crawl', datetime.datetime.now().strftime("%Y-%m-%dT%H-%M"))

os.makedirs(RUN_DIR, exist_ok=True)


def runCommand(*args, **kwargs):
    #return print(args, kwargs)
    return subprocess.run(*args, **kwargs)


def runSpider(spiderName, url, full=False):
    feedFile = f'{spiderName}.jl'
    feedPath = os.path.join(RUN_DIR, feedFile)
    logPath = os.path.join(RUN_DIR, f'{spiderName}.log')
    command = [
        #'echo',
        'scrapy',
        'crawl', spiderName,
        '--set', f'FEED_URI=file://{feedPath}',
        '--set', 'DELTAFETCH_ENABLED=1',
    ]
    if not full:
        command += [
            '--set', f'STOP_AFTER_SKIPPED={STOP_AFTER_SKIPPED}',
        ]
    if full:
        command += [
            '--set', 'DELTAFETCH_RESET=1'
        ]
    if url:
        command += ['--set', f'START_URL={url}']
    with open(logPath, 'a+') as log:
        log.write('Executing:' + str(command) + "\n")
        log.flush()
        runCommand(command, stdout=log, stderr=log)
    return feedPath


def importFeeds(feeds, indexOffsetDays=None):
    command = [
        #'echo',
        os.path.join(THIS_DIR, "import_feed"),
    ]
    if indexOffsetDays is not None:
        command = [
            '--index-date', (crawlDate - datetime.timedelta(days=indexOffsetDays)).date().isoformat()
        ]
    command += [feed for feed in feeds if os.path.exists(feed)]
    logPath = os.path.join(RUN_DIR, 'import.log')
    with open(logPath, 'a+') as log:
        log.write('Executing:' + str(command) + "\n")
        log.flush()
        runCommand(command, stdout=log, stderr=log)


usage = "usage: %prog <feed>"
description = "Run rentbot Crawlers"
parser = argparse.ArgumentParser(description=description)
parser.add_argument("-o", "--offset",
                    default=None, type=int,
                    help="Index offset in days")
args = parser.parse_args()


os.chdir(os.path.join(THIS_DIR, 'crawler'))
feeds = []
for spiderName, urls in SPIDERS.items():
    if urls:
        for url in urls:
            feedPath = runSpider(spiderName, url)
            feeds.append(feedPath)
    else:
        feeds = [runSpider(spiderName, None)]

os.chdir(RUN_DIR)
importFeeds(list(set(feeds)), args.offset)
