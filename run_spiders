#!/usr/bin/env python3.6

import os
import datetime
import subprocess
import argparse
import sys
import glob


ALL_SPIDERS_OPTS = ['HTTPCACHE_ENABLED=0']
SPIDERS = {
    'olx': None,
    'imot': None,
    'fbGroup': {
        'settings': [
            'ROBOTSTXT_OBEY=0',
            "USER_AGENT=Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:54.0) Gecko/20100101 Firefox/54.0"
        ],
        'urls': [
            '383690155099840/feed',
            '467058736744276/feed',
        ]
    }
}
STOP_AFTER_SKIPPED = 10

crawlDate = datetime.datetime.now()  # - datetime.timedelta(days=1)


THIS_DIR = os.path.dirname(os.path.realpath(__file__))
DATA_DIR = '/srv/'
if sys.platform == "darwin":
    DATA_DIR = os.path.join(os.getcwd(), '_test')


def runCommand(*args, **kwargs):
    logPath = kwargs.pop('logPath', None)
    if logPath:
        log = open(logPath, 'a+')
    else:
        log = None
    log.write('Executing:' + str(args[0]) + "\n")
    log.flush()
    kwargs['stdout'] = log
    kwargs['stderr'] = log
    if DRY_RUN:
        result = True
    else:
        result = subprocess.run(*args, **kwargs)
    log.flush()
    log.close()
    return result


def runSpider(spiderName, url, config, full):
    print(f'Running FULL={full} spider {spiderName} URL={url} ')
    feedFile = f'{spiderName}.jl'
    feedPath = os.path.join(RUN_DIR, feedFile)
    logPath = os.path.join(RUN_DIR, f'{spiderName}.log')
    command = [
        'scrapy',
        'crawl', spiderName,
        '--set', f'FEED_URI=file://{feedPath}',
        '--set', 'DELTAFETCH_ENABLED=1',
    ]
    for opt in ALL_SPIDERS_OPTS:
        command += [
            '--set', opt,
        ]
    if not full:
        command += [
            '--set', f'STOP_AFTER_SKIPPED={STOP_AFTER_SKIPPED}',
        ]
    if full:
        command += [
            '--set', 'DELTAFETCH_RESET=1'
        ]
    settings = config.get('settings', [])
    for setting in settings:
        command += [
            '--set', setting
        ]
    if url:
        command += ['--set', f'START_URL={url}']
    runCommand(command, logPath=logPath)
    return feedPath


def importFeeds(feeds, deleteOlder=False, indexOffsetDays=None):
    command = [
        os.path.join(THIS_DIR, "import_feed"),
    ]
    if indexOffsetDays is not None:
        command += [
            '--index-date', (crawlDate - datetime.timedelta(days=indexOffsetDays)).date().isoformat()
        ]
    if deleteOlder:
        command += ['--delete-older']
    command += [feed for feed in feeds if os.path.exists(feed)]
    logPath = os.path.join(RUN_DIR, 'import.log')
    runCommand(command, logPath=logPath)


def genSitemap():
    command = [
        os.path.join(THIS_DIR, "gen_sitemap"),
        os.path.join(THIS_DIR, 'web', 'build', 'sitemap')
    ]
    logPath = os.path.join(RUN_DIR, 'sitemap.log')
    runCommand(command, logPath=logPath)



usage = "usage: %prog <feed>"
description = "Run rentbot Crawlers"
parser = argparse.ArgumentParser(description=description)
parser.add_argument("--force-full",
                    action="store_true",
                    help="Force full scrape")
parser.add_argument("--force-incremental",
                    action="store_true",
                    help="Force incremental scrape")
parser.add_argument("--dry-run",
                    action="store_true",
                    help="Dry run, only print commands don't execute them")
parser.add_argument("--import-dir",
                    default=None,
                    help="Import *.jl files from dir, don't run spiders")
parser.add_argument("spider",
                    nargs="*",
                    help="spiders to run")
args = parser.parse_args()

spidersToRun = args.spider
if not spidersToRun:
    spidersToRun = SPIDERS.keys()

DRY_RUN = args.dry_run

# run full scrape between 00:00 and 01:00 on Sunday
TODAY = datetime.datetime.now()
FULL_SCRAPE = 0 <= TODAY.hour < 1 and TODAY.isoweekday() == 7

if args.force_incremental:
    FULL_SCRAPE = False

if args.force_full:
    FULL_SCRAPE = True

CRAWLS_DIR = os.path.join(DATA_DIR, 'crawl')
rdPrefix = 'full-' if FULL_SCRAPE else 'incr-'
RUN_DIR = os.path.join(CRAWLS_DIR, rdPrefix + datetime.datetime.now().strftime("%Y-%m-%dT%H-%M"))

os.makedirs(RUN_DIR, exist_ok=True)



feeds = []
if args.import_dir:
    feeds = glob.glob(os.path.join(args.import_dir, '*.jl'))
    feeds = [os.path.abspath(feed) for feed in feeds]
    if not feeds:
        print(f'No files found in {args.import_dir}. Exiting!')
        exit(-1)
else:
    os.chdir(os.path.join(THIS_DIR, 'crawler'))
    for spiderName in spidersToRun:
        if spiderName not in SPIDERS:
            print(f'Invalid spider name {spiderName}, ignoring')
            continue
        spiderConfig = SPIDERS[spiderName]
        config = spiderConfig or {}
        urls = config.get('urls', None)
        if urls:
            for url in urls:
                feedPath = runSpider(spiderName, url, config, FULL_SCRAPE)
                feeds.append(feedPath)
        else:
            feeds.append(runSpider(spiderName, None, config, FULL_SCRAPE))

os.chdir(RUN_DIR)
# Import in todays index
importFeeds(list(set(feeds)), FULL_SCRAPE, None)
genSitemap()
